"""Beam module."""

from __future__ import annotations

from collections.abc import Mapping
import typing
from typing import Any

from etils import epath

from mlcroissant._src.datasets import Dataset

if typing.TYPE_CHECKING:
    import apache_beam as beam


def ReadFromCroissant(
    *,
    pipeline: beam.Pipeline,
    jsonld: epath.PathLike | Mapping[str, Any],
    record_set: str,
    mapping: Mapping[str, epath.PathLike] | None = None,
):
    """Returns an Apache Beam reader to generate the dataset using e.g. Spark.

    Example of usage:

    ```python
    import apache_beam as beam
    from apache_beam.options import pipeline_options
    import mlcroissant as mlc

    jsonld = "https://huggingface.co/api/datasets/ylecun/mnist/croissant"

    options = pipeline_options.PipelineOptions()
    with beam.Pipeline(options=options) as pipeline:
        ReadFromCroissant(
            pipeline=pipeline,
            jsonld=jsonld,
            record_set="default",
        )
    ```

    Only streamable datasets can be used with Beam. A streamable dataset is a dataset
    that can be generated by a linear sequence of operations - without joins for
    example. This is the case for Hugging Face datasets. If there are branches, we'd
    need a more complex Beam pipeline.

    The sharding is done on the filtered files. This is currently optimized for Hugging
    Face datasets, so it raises an error if the dataset is not a Hugging Face dataset.

    Args:
        pipeline: A Beam pipeline.
        jsonld: A JSON object or a path to a Croissant file (URL, str or pathlib.Path).
        record_set: The name of the record set to generate.
        mapping: Mapping filename->filepath as a Python dict[str, str] to handle manual
            downloads. If `document.csv` is the FileObject and you downloaded it to
            `~/Downloads/document.csv`, you can specify `mapping={"document.csv":
            "~/Downloads/document.csv"}`.

    Returns:
        A Beam PCollection with all the records.

    Raises:
        A ValueError if the dataset is not streamable.
    """
    dataset = Dataset(jsonld=jsonld, mapping=mapping)
    return dataset.records(record_set).beam_reader(pipeline)
